{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of P2S10_Walker2DBulletEnv-v0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# Twin-Delayed DDPG\n",
        "\n",
        "Complete credit goes to this [awesome Deep Reinforcement Learning 2.0 Course on Udemy](https://www.udemy.com/course/deep-reinforcement-learning/) for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "outputId": "07cde0b7-9158-43f5-d856-b17586423330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.6/dist-packages (2.7.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex",
        "colab_type": "text"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"Walker2DBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "outputId": "63395cc7-6a51-4f97-9da2-ac2427d516d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_Walker2DBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O",
        "colab_type": "text"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "outputId": "238a48b6-19a6-44af-a09e-56f19b495133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh",
        "colab_type": "text"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s",
        "colab_type": "text"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "outputId": "629bebf1-2c25-489d-858b-68daca0c4bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 264.967772\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE",
        "colab_type": "text"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "outputId": "760d01f4-4f87-4760-dee4-5a115532786d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_timesteps = 500000\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 13 Episode Num: 1 Reward: 16.709465533394543\n",
            "Total Timesteps: 22 Episode Num: 2 Reward: 13.40157706606551\n",
            "Total Timesteps: 32 Episode Num: 3 Reward: 14.56838037951966\n",
            "Total Timesteps: 50 Episode Num: 4 Reward: 17.788174748278106\n",
            "Total Timesteps: 58 Episode Num: 5 Reward: 12.860323792080452\n",
            "Total Timesteps: 66 Episode Num: 6 Reward: 12.556321013024716\n",
            "Total Timesteps: 81 Episode Num: 7 Reward: 16.69448528290086\n",
            "Total Timesteps: 98 Episode Num: 8 Reward: 16.993699231353823\n",
            "Total Timesteps: 120 Episode Num: 9 Reward: 19.862857718774467\n",
            "Total Timesteps: 135 Episode Num: 10 Reward: 16.305328237384675\n",
            "Total Timesteps: 155 Episode Num: 11 Reward: 21.84359003383288\n",
            "Total Timesteps: 166 Episode Num: 12 Reward: 16.8597156313117\n",
            "Total Timesteps: 183 Episode Num: 13 Reward: 18.75913506756769\n",
            "Total Timesteps: 198 Episode Num: 14 Reward: 18.891420575758094\n",
            "Total Timesteps: 212 Episode Num: 15 Reward: 15.397715638121008\n",
            "Total Timesteps: 224 Episode Num: 16 Reward: 10.846338063343136\n",
            "Total Timesteps: 246 Episode Num: 17 Reward: 20.060308664172776\n",
            "Total Timesteps: 258 Episode Num: 18 Reward: 14.889976311005011\n",
            "Total Timesteps: 271 Episode Num: 19 Reward: 17.060068034319553\n",
            "Total Timesteps: 283 Episode Num: 20 Reward: 15.550589209432657\n",
            "Total Timesteps: 295 Episode Num: 21 Reward: 13.763838627676886\n",
            "Total Timesteps: 309 Episode Num: 22 Reward: 16.530021560221215\n",
            "Total Timesteps: 320 Episode Num: 23 Reward: 14.953327815176452\n",
            "Total Timesteps: 329 Episode Num: 24 Reward: 13.536216865388267\n",
            "Total Timesteps: 345 Episode Num: 25 Reward: 16.487241053835895\n",
            "Total Timesteps: 356 Episode Num: 26 Reward: 13.869942831387743\n",
            "Total Timesteps: 368 Episode Num: 27 Reward: 15.136660653410944\n",
            "Total Timesteps: 390 Episode Num: 28 Reward: 21.429516966598747\n",
            "Total Timesteps: 398 Episode Num: 29 Reward: 12.04653724025702\n",
            "Total Timesteps: 405 Episode Num: 30 Reward: 11.83407155227178\n",
            "Total Timesteps: 414 Episode Num: 31 Reward: 13.075095655448967\n",
            "Total Timesteps: 428 Episode Num: 32 Reward: 16.653103546783676\n",
            "Total Timesteps: 439 Episode Num: 33 Reward: 15.371692239708498\n",
            "Total Timesteps: 447 Episode Num: 34 Reward: 11.256802543025696\n",
            "Total Timesteps: 459 Episode Num: 35 Reward: 15.001758714436438\n",
            "Total Timesteps: 469 Episode Num: 36 Reward: 14.8460660969562\n",
            "Total Timesteps: 480 Episode Num: 37 Reward: 14.33898116526252\n",
            "Total Timesteps: 500 Episode Num: 38 Reward: 20.640147303104463\n",
            "Total Timesteps: 516 Episode Num: 39 Reward: 19.163991850370074\n",
            "Total Timesteps: 540 Episode Num: 40 Reward: 25.08983557369647\n",
            "Total Timesteps: 559 Episode Num: 41 Reward: 16.985948069603186\n",
            "Total Timesteps: 572 Episode Num: 42 Reward: 17.108626215114782\n",
            "Total Timesteps: 583 Episode Num: 43 Reward: 14.88016055279586\n",
            "Total Timesteps: 598 Episode Num: 44 Reward: 18.705623487595584\n",
            "Total Timesteps: 612 Episode Num: 45 Reward: 18.574912262219005\n",
            "Total Timesteps: 622 Episode Num: 46 Reward: 13.548265837300278\n",
            "Total Timesteps: 632 Episode Num: 47 Reward: 11.687981341233533\n",
            "Total Timesteps: 644 Episode Num: 48 Reward: 16.891381838012602\n",
            "Total Timesteps: 668 Episode Num: 49 Reward: 22.550196672375023\n",
            "Total Timesteps: 677 Episode Num: 50 Reward: 13.369306103838609\n",
            "Total Timesteps: 701 Episode Num: 51 Reward: 19.389304192907\n",
            "Total Timesteps: 712 Episode Num: 52 Reward: 15.58963848977146\n",
            "Total Timesteps: 740 Episode Num: 53 Reward: 27.621845678071264\n",
            "Total Timesteps: 754 Episode Num: 54 Reward: 17.66060964630451\n",
            "Total Timesteps: 767 Episode Num: 55 Reward: 15.667187877581455\n",
            "Total Timesteps: 776 Episode Num: 56 Reward: 13.978284573154816\n",
            "Total Timesteps: 800 Episode Num: 57 Reward: 22.876505151645688\n",
            "Total Timesteps: 816 Episode Num: 58 Reward: 17.106124479282883\n",
            "Total Timesteps: 825 Episode Num: 59 Reward: 13.679223671309591\n",
            "Total Timesteps: 842 Episode Num: 60 Reward: 15.92607861391298\n",
            "Total Timesteps: 853 Episode Num: 61 Reward: 15.0760073287689\n",
            "Total Timesteps: 866 Episode Num: 62 Reward: 16.588852422690252\n",
            "Total Timesteps: 875 Episode Num: 63 Reward: 13.91587152117281\n",
            "Total Timesteps: 888 Episode Num: 64 Reward: 16.751431654670156\n",
            "Total Timesteps: 895 Episode Num: 65 Reward: 11.090949352865572\n",
            "Total Timesteps: 905 Episode Num: 66 Reward: 13.657382863105159\n",
            "Total Timesteps: 919 Episode Num: 67 Reward: 15.175601682945853\n",
            "Total Timesteps: 932 Episode Num: 68 Reward: 17.214798125863307\n",
            "Total Timesteps: 954 Episode Num: 69 Reward: 23.11889218765427\n",
            "Total Timesteps: 969 Episode Num: 70 Reward: 17.184553285474248\n",
            "Total Timesteps: 990 Episode Num: 71 Reward: 22.05627120063145\n",
            "Total Timesteps: 1010 Episode Num: 72 Reward: 17.976451307366368\n",
            "Total Timesteps: 1017 Episode Num: 73 Reward: 11.491681034087378\n",
            "Total Timesteps: 1032 Episode Num: 74 Reward: 18.103386047744426\n",
            "Total Timesteps: 1040 Episode Num: 75 Reward: 12.243811202542565\n",
            "Total Timesteps: 1062 Episode Num: 76 Reward: 24.221807990448728\n",
            "Total Timesteps: 1074 Episode Num: 77 Reward: 16.119098578969716\n",
            "Total Timesteps: 1089 Episode Num: 78 Reward: 17.887322138869784\n",
            "Total Timesteps: 1099 Episode Num: 79 Reward: 14.346644599304998\n",
            "Total Timesteps: 1116 Episode Num: 80 Reward: 16.344172743760282\n",
            "Total Timesteps: 1127 Episode Num: 81 Reward: 14.351996076482465\n",
            "Total Timesteps: 1138 Episode Num: 82 Reward: 15.307921445801915\n",
            "Total Timesteps: 1148 Episode Num: 83 Reward: 12.673478086489196\n",
            "Total Timesteps: 1167 Episode Num: 84 Reward: 18.799088276817926\n",
            "Total Timesteps: 1183 Episode Num: 85 Reward: 19.209376800328023\n",
            "Total Timesteps: 1202 Episode Num: 86 Reward: 21.38692322864808\n",
            "Total Timesteps: 1211 Episode Num: 87 Reward: 13.160237324752961\n",
            "Total Timesteps: 1230 Episode Num: 88 Reward: 18.653688868985046\n",
            "Total Timesteps: 1241 Episode Num: 89 Reward: 15.282569505015273\n",
            "Total Timesteps: 1261 Episode Num: 90 Reward: 18.901490309956714\n",
            "Total Timesteps: 1275 Episode Num: 91 Reward: 15.618259488904732\n",
            "Total Timesteps: 1286 Episode Num: 92 Reward: 15.202514871722087\n",
            "Total Timesteps: 1310 Episode Num: 93 Reward: 24.398208765589516\n",
            "Total Timesteps: 1325 Episode Num: 94 Reward: 16.346027853515984\n",
            "Total Timesteps: 1334 Episode Num: 95 Reward: 13.4887173126408\n",
            "Total Timesteps: 1343 Episode Num: 96 Reward: 13.880933258887673\n",
            "Total Timesteps: 1350 Episode Num: 97 Reward: 12.370617874404706\n",
            "Total Timesteps: 1358 Episode Num: 98 Reward: 12.514647518284619\n",
            "Total Timesteps: 1373 Episode Num: 99 Reward: 17.093242570463918\n",
            "Total Timesteps: 1384 Episode Num: 100 Reward: 16.380371760805428\n",
            "Total Timesteps: 1400 Episode Num: 101 Reward: 17.80969796680583\n",
            "Total Timesteps: 1415 Episode Num: 102 Reward: 16.530795927530562\n",
            "Total Timesteps: 1433 Episode Num: 103 Reward: 18.021070372163376\n",
            "Total Timesteps: 1443 Episode Num: 104 Reward: 13.62490386275749\n",
            "Total Timesteps: 1452 Episode Num: 105 Reward: 13.491261619111174\n",
            "Total Timesteps: 1461 Episode Num: 106 Reward: 13.727746808392112\n",
            "Total Timesteps: 1470 Episode Num: 107 Reward: 13.58775206970895\n",
            "Total Timesteps: 1483 Episode Num: 108 Reward: 12.676353485891013\n",
            "Total Timesteps: 1490 Episode Num: 109 Reward: 11.976777879676956\n",
            "Total Timesteps: 1498 Episode Num: 110 Reward: 12.837510337209096\n",
            "Total Timesteps: 1509 Episode Num: 111 Reward: 14.262726259620104\n",
            "Total Timesteps: 1522 Episode Num: 112 Reward: 17.31213005559403\n",
            "Total Timesteps: 1535 Episode Num: 113 Reward: 16.21446814217925\n",
            "Total Timesteps: 1549 Episode Num: 114 Reward: 16.76843535751686\n",
            "Total Timesteps: 1564 Episode Num: 115 Reward: 15.850019316436372\n",
            "Total Timesteps: 1573 Episode Num: 116 Reward: 13.68313602592534\n",
            "Total Timesteps: 1586 Episode Num: 117 Reward: 13.68629365698289\n",
            "Total Timesteps: 1598 Episode Num: 118 Reward: 16.317168168540228\n",
            "Total Timesteps: 1612 Episode Num: 119 Reward: 17.82783647277538\n",
            "Total Timesteps: 1631 Episode Num: 120 Reward: 21.54984030192718\n",
            "Total Timesteps: 1646 Episode Num: 121 Reward: 17.75390878330363\n",
            "Total Timesteps: 1655 Episode Num: 122 Reward: 14.868617843264655\n",
            "Total Timesteps: 1680 Episode Num: 123 Reward: 25.057114175426246\n",
            "Total Timesteps: 1701 Episode Num: 124 Reward: 21.444209690406566\n",
            "Total Timesteps: 1710 Episode Num: 125 Reward: 14.796222879238483\n",
            "Total Timesteps: 1726 Episode Num: 126 Reward: 17.428999921426293\n",
            "Total Timesteps: 1743 Episode Num: 127 Reward: 18.635105693065274\n",
            "Total Timesteps: 1752 Episode Num: 128 Reward: 13.807591038371903\n",
            "Total Timesteps: 1762 Episode Num: 129 Reward: 12.503200505806308\n",
            "Total Timesteps: 1775 Episode Num: 130 Reward: 15.95263407045422\n",
            "Total Timesteps: 1791 Episode Num: 131 Reward: 18.198649979190666\n",
            "Total Timesteps: 1808 Episode Num: 132 Reward: 19.389376898862242\n",
            "Total Timesteps: 1824 Episode Num: 133 Reward: 18.171548979810904\n",
            "Total Timesteps: 1832 Episode Num: 134 Reward: 12.052215297878137\n",
            "Total Timesteps: 1852 Episode Num: 135 Reward: 19.177475691874857\n",
            "Total Timesteps: 1868 Episode Num: 136 Reward: 16.78485830087157\n",
            "Total Timesteps: 1879 Episode Num: 137 Reward: 14.391437246080024\n",
            "Total Timesteps: 1889 Episode Num: 138 Reward: 16.209492007859808\n",
            "Total Timesteps: 1901 Episode Num: 139 Reward: 14.042251200365714\n",
            "Total Timesteps: 1921 Episode Num: 140 Reward: 21.719475400050577\n",
            "Total Timesteps: 1930 Episode Num: 141 Reward: 13.379466437270455\n",
            "Total Timesteps: 1948 Episode Num: 142 Reward: 15.610280352452538\n",
            "Total Timesteps: 1957 Episode Num: 143 Reward: 12.978456888454094\n",
            "Total Timesteps: 1966 Episode Num: 144 Reward: 13.131181443964307\n",
            "Total Timesteps: 1989 Episode Num: 145 Reward: 21.56438537542708\n",
            "Total Timesteps: 2000 Episode Num: 146 Reward: 13.527462393784663\n",
            "Total Timesteps: 2012 Episode Num: 147 Reward: 14.877820546590373\n",
            "Total Timesteps: 2033 Episode Num: 148 Reward: 20.722173570661106\n",
            "Total Timesteps: 2044 Episode Num: 149 Reward: 16.190875416388735\n",
            "Total Timesteps: 2059 Episode Num: 150 Reward: 17.304405149041852\n",
            "Total Timesteps: 2066 Episode Num: 151 Reward: 11.3708579459897\n",
            "Total Timesteps: 2084 Episode Num: 152 Reward: 18.418253502102743\n",
            "Total Timesteps: 2106 Episode Num: 153 Reward: 22.314945717000228\n",
            "Total Timesteps: 2120 Episode Num: 154 Reward: 15.480506800822333\n",
            "Total Timesteps: 2129 Episode Num: 155 Reward: 12.766867294737311\n",
            "Total Timesteps: 2143 Episode Num: 156 Reward: 19.59736371721083\n",
            "Total Timesteps: 2157 Episode Num: 157 Reward: 16.79660952382983\n",
            "Total Timesteps: 2169 Episode Num: 158 Reward: 15.619335710794253\n",
            "Total Timesteps: 2186 Episode Num: 159 Reward: 18.543630527511414\n",
            "Total Timesteps: 2193 Episode Num: 160 Reward: 12.414205489186859\n",
            "Total Timesteps: 2207 Episode Num: 161 Reward: 16.912357516941846\n",
            "Total Timesteps: 2221 Episode Num: 162 Reward: 16.15991274622356\n",
            "Total Timesteps: 2233 Episode Num: 163 Reward: 15.04219860415178\n",
            "Total Timesteps: 2243 Episode Num: 164 Reward: 13.076527605461889\n",
            "Total Timesteps: 2260 Episode Num: 165 Reward: 17.53361420404108\n",
            "Total Timesteps: 2269 Episode Num: 166 Reward: 13.620694738418388\n",
            "Total Timesteps: 2281 Episode Num: 167 Reward: 15.155654646277252\n",
            "Total Timesteps: 2293 Episode Num: 168 Reward: 15.628605340298963\n",
            "Total Timesteps: 2305 Episode Num: 169 Reward: 16.689786490486586\n",
            "Total Timesteps: 2315 Episode Num: 170 Reward: 14.307139940184424\n",
            "Total Timesteps: 2324 Episode Num: 171 Reward: 13.991717384697406\n",
            "Total Timesteps: 2336 Episode Num: 172 Reward: 16.096497071655175\n",
            "Total Timesteps: 2346 Episode Num: 173 Reward: 14.203701349678155\n",
            "Total Timesteps: 2377 Episode Num: 174 Reward: 30.02222433575225\n",
            "Total Timesteps: 2394 Episode Num: 175 Reward: 17.207982422725763\n",
            "Total Timesteps: 2405 Episode Num: 176 Reward: 13.857478436238308\n",
            "Total Timesteps: 2423 Episode Num: 177 Reward: 23.02863181007124\n",
            "Total Timesteps: 2436 Episode Num: 178 Reward: 14.780061887955526\n",
            "Total Timesteps: 2445 Episode Num: 179 Reward: 13.989998195941734\n",
            "Total Timesteps: 2457 Episode Num: 180 Reward: 14.53872979129519\n",
            "Total Timesteps: 2464 Episode Num: 181 Reward: 13.218552665818425\n",
            "Total Timesteps: 2478 Episode Num: 182 Reward: 15.754311670380414\n",
            "Total Timesteps: 2489 Episode Num: 183 Reward: 14.235645655018741\n",
            "Total Timesteps: 2505 Episode Num: 184 Reward: 19.026652753890087\n",
            "Total Timesteps: 2516 Episode Num: 185 Reward: 14.630797690823965\n",
            "Total Timesteps: 2538 Episode Num: 186 Reward: 21.390825525707623\n",
            "Total Timesteps: 2547 Episode Num: 187 Reward: 12.534054751627263\n",
            "Total Timesteps: 2568 Episode Num: 188 Reward: 19.06934537265624\n",
            "Total Timesteps: 2576 Episode Num: 189 Reward: 12.886151999139111\n",
            "Total Timesteps: 2589 Episode Num: 190 Reward: 15.347098243431535\n",
            "Total Timesteps: 2607 Episode Num: 191 Reward: 21.09665797031339\n",
            "Total Timesteps: 2618 Episode Num: 192 Reward: 14.046517469731043\n",
            "Total Timesteps: 2643 Episode Num: 193 Reward: 21.33027822479053\n",
            "Total Timesteps: 2653 Episode Num: 194 Reward: 14.539134049686254\n",
            "Total Timesteps: 2666 Episode Num: 195 Reward: 17.806861554854546\n",
            "Total Timesteps: 2685 Episode Num: 196 Reward: 19.984070620483543\n",
            "Total Timesteps: 2695 Episode Num: 197 Reward: 14.60618690941337\n",
            "Total Timesteps: 2715 Episode Num: 198 Reward: 20.62494029302761\n",
            "Total Timesteps: 2726 Episode Num: 199 Reward: 15.513985623166082\n",
            "Total Timesteps: 2740 Episode Num: 200 Reward: 18.335935251788765\n",
            "Total Timesteps: 2753 Episode Num: 201 Reward: 15.679291607830963\n",
            "Total Timesteps: 2769 Episode Num: 202 Reward: 17.782614974178433\n",
            "Total Timesteps: 2783 Episode Num: 203 Reward: 15.358997436144273\n",
            "Total Timesteps: 2800 Episode Num: 204 Reward: 17.87132388430182\n",
            "Total Timesteps: 2817 Episode Num: 205 Reward: 16.833868647432244\n",
            "Total Timesteps: 2830 Episode Num: 206 Reward: 15.635696960508358\n",
            "Total Timesteps: 2841 Episode Num: 207 Reward: 14.629497186819206\n",
            "Total Timesteps: 2855 Episode Num: 208 Reward: 15.360315885810994\n",
            "Total Timesteps: 2872 Episode Num: 209 Reward: 17.170070912686057\n",
            "Total Timesteps: 2883 Episode Num: 210 Reward: 15.775497221229307\n",
            "Total Timesteps: 2898 Episode Num: 211 Reward: 17.767793441409598\n",
            "Total Timesteps: 2912 Episode Num: 212 Reward: 19.1445496672517\n",
            "Total Timesteps: 2925 Episode Num: 213 Reward: 15.598188406671396\n",
            "Total Timesteps: 2936 Episode Num: 214 Reward: 15.365932353225073\n",
            "Total Timesteps: 2944 Episode Num: 215 Reward: 12.320142193269566\n",
            "Total Timesteps: 2964 Episode Num: 216 Reward: 22.073992228192214\n",
            "Total Timesteps: 2972 Episode Num: 217 Reward: 12.916223937814358\n",
            "Total Timesteps: 2982 Episode Num: 218 Reward: 14.614077253444702\n",
            "Total Timesteps: 2994 Episode Num: 219 Reward: 15.701291930877778\n",
            "Total Timesteps: 3007 Episode Num: 220 Reward: 16.31883260788163\n",
            "Total Timesteps: 3024 Episode Num: 221 Reward: 19.25645379569469\n",
            "Total Timesteps: 3042 Episode Num: 222 Reward: 17.941475293744585\n",
            "Total Timesteps: 3050 Episode Num: 223 Reward: 13.155216057080544\n",
            "Total Timesteps: 3057 Episode Num: 224 Reward: 12.630540289844793\n",
            "Total Timesteps: 3076 Episode Num: 225 Reward: 18.05914827611268\n",
            "Total Timesteps: 3088 Episode Num: 226 Reward: 15.373530031916744\n",
            "Total Timesteps: 3110 Episode Num: 227 Reward: 21.24517186428129\n",
            "Total Timesteps: 3120 Episode Num: 228 Reward: 14.129773531487446\n",
            "Total Timesteps: 3132 Episode Num: 229 Reward: 15.58358384911844\n",
            "Total Timesteps: 3147 Episode Num: 230 Reward: 14.07627487050777\n",
            "Total Timesteps: 3181 Episode Num: 231 Reward: 20.243304109539896\n",
            "Total Timesteps: 3193 Episode Num: 232 Reward: 16.50411732428329\n",
            "Total Timesteps: 3202 Episode Num: 233 Reward: 13.600966412927665\n",
            "Total Timesteps: 3218 Episode Num: 234 Reward: 20.446063365974986\n",
            "Total Timesteps: 3233 Episode Num: 235 Reward: 17.139097625880098\n",
            "Total Timesteps: 3250 Episode Num: 236 Reward: 17.567811932362382\n",
            "Total Timesteps: 3271 Episode Num: 237 Reward: 18.21603903838404\n",
            "Total Timesteps: 3280 Episode Num: 238 Reward: 13.307268045395903\n",
            "Total Timesteps: 3292 Episode Num: 239 Reward: 16.050123255387003\n",
            "Total Timesteps: 3306 Episode Num: 240 Reward: 17.0952240577215\n",
            "Total Timesteps: 3322 Episode Num: 241 Reward: 17.349906205647855\n",
            "Total Timesteps: 3342 Episode Num: 242 Reward: 20.12132459350978\n",
            "Total Timesteps: 3370 Episode Num: 243 Reward: 28.128586070929305\n",
            "Total Timesteps: 3380 Episode Num: 244 Reward: 14.229904883114795\n",
            "Total Timesteps: 3394 Episode Num: 245 Reward: 18.245264947571556\n",
            "Total Timesteps: 3405 Episode Num: 246 Reward: 15.964277462617611\n",
            "Total Timesteps: 3414 Episode Num: 247 Reward: 13.135705251916079\n",
            "Total Timesteps: 3429 Episode Num: 248 Reward: 20.575115740999173\n",
            "Total Timesteps: 3450 Episode Num: 249 Reward: 20.726560184206754\n",
            "Total Timesteps: 3467 Episode Num: 250 Reward: 19.891895367499096\n",
            "Total Timesteps: 3491 Episode Num: 251 Reward: 22.608911268057998\n",
            "Total Timesteps: 3509 Episode Num: 252 Reward: 19.181746429974734\n",
            "Total Timesteps: 3517 Episode Num: 253 Reward: 11.506324471672999\n",
            "Total Timesteps: 3533 Episode Num: 254 Reward: 15.2677959990644\n",
            "Total Timesteps: 3543 Episode Num: 255 Reward: 12.851634943204406\n",
            "Total Timesteps: 3563 Episode Num: 256 Reward: 22.054827261652104\n",
            "Total Timesteps: 3583 Episode Num: 257 Reward: 20.984416533479816\n",
            "Total Timesteps: 3590 Episode Num: 258 Reward: 12.996250190629508\n",
            "Total Timesteps: 3607 Episode Num: 259 Reward: 21.250978780144944\n",
            "Total Timesteps: 3620 Episode Num: 260 Reward: 15.933255123216076\n",
            "Total Timesteps: 3634 Episode Num: 261 Reward: 17.588309241866227\n",
            "Total Timesteps: 3650 Episode Num: 262 Reward: 18.38411134527123\n",
            "Total Timesteps: 3661 Episode Num: 263 Reward: 13.455443873954936\n",
            "Total Timesteps: 3671 Episode Num: 264 Reward: 14.774868827765747\n",
            "Total Timesteps: 3685 Episode Num: 265 Reward: 16.92270906639169\n",
            "Total Timesteps: 3701 Episode Num: 266 Reward: 18.15095598176558\n",
            "Total Timesteps: 3715 Episode Num: 267 Reward: 16.51745221555757\n",
            "Total Timesteps: 3730 Episode Num: 268 Reward: 17.6269937333971\n",
            "Total Timesteps: 3744 Episode Num: 269 Reward: 13.073728864813164\n",
            "Total Timesteps: 3754 Episode Num: 270 Reward: 13.286226401301974\n",
            "Total Timesteps: 3768 Episode Num: 271 Reward: 19.58245159490034\n",
            "Total Timesteps: 3779 Episode Num: 272 Reward: 14.249112128415433\n",
            "Total Timesteps: 3799 Episode Num: 273 Reward: 18.907782259976376\n",
            "Total Timesteps: 3817 Episode Num: 274 Reward: 16.073281542865153\n",
            "Total Timesteps: 3831 Episode Num: 275 Reward: 17.910217491729416\n",
            "Total Timesteps: 3842 Episode Num: 276 Reward: 16.08782425098907\n",
            "Total Timesteps: 3856 Episode Num: 277 Reward: 15.259550009941451\n",
            "Total Timesteps: 3867 Episode Num: 278 Reward: 15.237016288626183\n",
            "Total Timesteps: 3889 Episode Num: 279 Reward: 19.117164927424167\n",
            "Total Timesteps: 3905 Episode Num: 280 Reward: 15.89763007950096\n",
            "Total Timesteps: 3919 Episode Num: 281 Reward: 15.709096252478775\n",
            "Total Timesteps: 3938 Episode Num: 282 Reward: 15.802347402441956\n",
            "Total Timesteps: 3960 Episode Num: 283 Reward: 24.129996467431194\n",
            "Total Timesteps: 3976 Episode Num: 284 Reward: 18.886843061212858\n",
            "Total Timesteps: 3987 Episode Num: 285 Reward: 13.26251161381224\n",
            "Total Timesteps: 3996 Episode Num: 286 Reward: 13.92849836925161\n",
            "Total Timesteps: 4016 Episode Num: 287 Reward: 21.580620381984044\n",
            "Total Timesteps: 4034 Episode Num: 288 Reward: 21.471509248153595\n",
            "Total Timesteps: 4050 Episode Num: 289 Reward: 15.737283577585185\n",
            "Total Timesteps: 4065 Episode Num: 290 Reward: 15.858090299989271\n",
            "Total Timesteps: 4080 Episode Num: 291 Reward: 17.761724825238343\n",
            "Total Timesteps: 4090 Episode Num: 292 Reward: 13.471629692148417\n",
            "Total Timesteps: 4108 Episode Num: 293 Reward: 15.419905885674233\n",
            "Total Timesteps: 4121 Episode Num: 294 Reward: 15.780378145247232\n",
            "Total Timesteps: 4140 Episode Num: 295 Reward: 20.69792559404741\n",
            "Total Timesteps: 4150 Episode Num: 296 Reward: 14.16719016028801\n",
            "Total Timesteps: 4164 Episode Num: 297 Reward: 17.874390590094844\n",
            "Total Timesteps: 4177 Episode Num: 298 Reward: 15.377334732117014\n",
            "Total Timesteps: 4185 Episode Num: 299 Reward: 13.021232648903968\n",
            "Total Timesteps: 4202 Episode Num: 300 Reward: 17.789535101063663\n",
            "Total Timesteps: 4220 Episode Num: 301 Reward: 22.07070002547407\n",
            "Total Timesteps: 4241 Episode Num: 302 Reward: 23.14406680220273\n",
            "Total Timesteps: 4257 Episode Num: 303 Reward: 17.81381406228611\n",
            "Total Timesteps: 4268 Episode Num: 304 Reward: 14.87861227364483\n",
            "Total Timesteps: 4281 Episode Num: 305 Reward: 14.685428236414735\n",
            "Total Timesteps: 4298 Episode Num: 306 Reward: 17.844480687979377\n",
            "Total Timesteps: 4308 Episode Num: 307 Reward: 15.112408869952198\n",
            "Total Timesteps: 4320 Episode Num: 308 Reward: 15.528294134145836\n",
            "Total Timesteps: 4336 Episode Num: 309 Reward: 17.82104354567855\n",
            "Total Timesteps: 4355 Episode Num: 310 Reward: 20.897951067806574\n",
            "Total Timesteps: 4365 Episode Num: 311 Reward: 14.252578629013442\n",
            "Total Timesteps: 4379 Episode Num: 312 Reward: 15.711677910971046\n",
            "Total Timesteps: 4394 Episode Num: 313 Reward: 17.190828422788762\n",
            "Total Timesteps: 4403 Episode Num: 314 Reward: 12.901742620728326\n",
            "Total Timesteps: 4420 Episode Num: 315 Reward: 15.833175660746928\n",
            "Total Timesteps: 4431 Episode Num: 316 Reward: 15.394466967243352\n",
            "Total Timesteps: 4442 Episode Num: 317 Reward: 15.033974140229112\n",
            "Total Timesteps: 4456 Episode Num: 318 Reward: 17.485353989404395\n",
            "Total Timesteps: 4475 Episode Num: 319 Reward: 19.399912562125245\n",
            "Total Timesteps: 4491 Episode Num: 320 Reward: 19.153693365558865\n",
            "Total Timesteps: 4503 Episode Num: 321 Reward: 15.64539509324095\n",
            "Total Timesteps: 4519 Episode Num: 322 Reward: 19.428761438786747\n",
            "Total Timesteps: 4551 Episode Num: 323 Reward: 30.773406511207575\n",
            "Total Timesteps: 4577 Episode Num: 324 Reward: 25.986071893622285\n",
            "Total Timesteps: 4588 Episode Num: 325 Reward: 16.286195503822817\n",
            "Total Timesteps: 4600 Episode Num: 326 Reward: 14.023557388088376\n",
            "Total Timesteps: 4615 Episode Num: 327 Reward: 16.626144796177687\n",
            "Total Timesteps: 4623 Episode Num: 328 Reward: 12.52080162971688\n",
            "Total Timesteps: 4638 Episode Num: 329 Reward: 15.34978915422398\n",
            "Total Timesteps: 4650 Episode Num: 330 Reward: 16.157998684859194\n",
            "Total Timesteps: 4671 Episode Num: 331 Reward: 21.39656679285836\n",
            "Total Timesteps: 4685 Episode Num: 332 Reward: 16.730134791605813\n",
            "Total Timesteps: 4694 Episode Num: 333 Reward: 13.860559666718471\n",
            "Total Timesteps: 4711 Episode Num: 334 Reward: 19.110486077584206\n",
            "Total Timesteps: 4734 Episode Num: 335 Reward: 21.29287829559762\n",
            "Total Timesteps: 4761 Episode Num: 336 Reward: 25.54543881671416\n",
            "Total Timesteps: 4777 Episode Num: 337 Reward: 19.184430452343076\n",
            "Total Timesteps: 4799 Episode Num: 338 Reward: 19.92752898294129\n",
            "Total Timesteps: 4808 Episode Num: 339 Reward: 15.14246983275807\n",
            "Total Timesteps: 4820 Episode Num: 340 Reward: 15.373110139725028\n",
            "Total Timesteps: 4834 Episode Num: 341 Reward: 17.343317867188308\n",
            "Total Timesteps: 4849 Episode Num: 342 Reward: 19.478898038581246\n",
            "Total Timesteps: 4859 Episode Num: 343 Reward: 13.306690168069327\n",
            "Total Timesteps: 4885 Episode Num: 344 Reward: 21.485542137427547\n",
            "Total Timesteps: 4899 Episode Num: 345 Reward: 14.416056607592326\n",
            "Total Timesteps: 4907 Episode Num: 346 Reward: 13.586993158741098\n",
            "Total Timesteps: 4915 Episode Num: 347 Reward: 13.022741167605272\n",
            "Total Timesteps: 4922 Episode Num: 348 Reward: 12.073390099353855\n",
            "Total Timesteps: 4939 Episode Num: 349 Reward: 16.327673536629298\n",
            "Total Timesteps: 4956 Episode Num: 350 Reward: 19.9524574104551\n",
            "Total Timesteps: 4972 Episode Num: 351 Reward: 18.98218752105313\n",
            "Total Timesteps: 4987 Episode Num: 352 Reward: 17.098271758979536\n",
            "Total Timesteps: 4998 Episode Num: 353 Reward: 15.069928995383087\n",
            "Total Timesteps: 5010 Episode Num: 354 Reward: 16.20412436597544\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 55.587329\n",
            "---------------------------------------\n",
            "Total Timesteps: 5018 Episode Num: 355 Reward: 12.630473384620563\n",
            "Total Timesteps: 5031 Episode Num: 356 Reward: 14.92387193970935\n",
            "Total Timesteps: 5042 Episode Num: 357 Reward: 14.305288477311842\n",
            "Total Timesteps: 5053 Episode Num: 358 Reward: 15.135454769880743\n",
            "Total Timesteps: 5064 Episode Num: 359 Reward: 14.108984607478487\n",
            "Total Timesteps: 5077 Episode Num: 360 Reward: 15.160570454478147\n",
            "Total Timesteps: 5091 Episode Num: 361 Reward: 18.529149965125548\n",
            "Total Timesteps: 5102 Episode Num: 362 Reward: 15.839366405043982\n",
            "Total Timesteps: 5114 Episode Num: 363 Reward: 16.03713619588525\n",
            "Total Timesteps: 5128 Episode Num: 364 Reward: 15.821579052062589\n",
            "Total Timesteps: 5151 Episode Num: 365 Reward: 23.94139742969564\n",
            "Total Timesteps: 5168 Episode Num: 366 Reward: 15.492122122579897\n",
            "Total Timesteps: 5185 Episode Num: 367 Reward: 18.393366993976816\n",
            "Total Timesteps: 5196 Episode Num: 368 Reward: 15.742353394303063\n",
            "Total Timesteps: 5206 Episode Num: 369 Reward: 14.460866179665025\n",
            "Total Timesteps: 5219 Episode Num: 370 Reward: 17.830283636380045\n",
            "Total Timesteps: 5238 Episode Num: 371 Reward: 18.561646960373036\n",
            "Total Timesteps: 5249 Episode Num: 372 Reward: 15.351080476814237\n",
            "Total Timesteps: 5262 Episode Num: 373 Reward: 15.809459711911039\n",
            "Total Timesteps: 5271 Episode Num: 374 Reward: 12.192213382519547\n",
            "Total Timesteps: 5287 Episode Num: 375 Reward: 19.171509348512334\n",
            "Total Timesteps: 5303 Episode Num: 376 Reward: 17.131096788987634\n",
            "Total Timesteps: 5318 Episode Num: 377 Reward: 16.772791077241706\n",
            "Total Timesteps: 5329 Episode Num: 378 Reward: 14.898653181229017\n",
            "Total Timesteps: 5341 Episode Num: 379 Reward: 15.30674388056941\n",
            "Total Timesteps: 5355 Episode Num: 380 Reward: 17.36376476067526\n",
            "Total Timesteps: 5375 Episode Num: 381 Reward: 17.949079728301147\n",
            "Total Timesteps: 5382 Episode Num: 382 Reward: 12.726437214690668\n",
            "Total Timesteps: 5400 Episode Num: 383 Reward: 20.02781872814812\n",
            "Total Timesteps: 5413 Episode Num: 384 Reward: 17.524191213645096\n",
            "Total Timesteps: 5421 Episode Num: 385 Reward: 12.869799177187087\n",
            "Total Timesteps: 5431 Episode Num: 386 Reward: 13.541083696665007\n",
            "Total Timesteps: 5444 Episode Num: 387 Reward: 15.087718818058782\n",
            "Total Timesteps: 5456 Episode Num: 388 Reward: 16.388495457416866\n",
            "Total Timesteps: 5491 Episode Num: 389 Reward: 29.583168622448287\n",
            "Total Timesteps: 5507 Episode Num: 390 Reward: 19.887934114171365\n",
            "Total Timesteps: 5518 Episode Num: 391 Reward: 14.378776965649744\n",
            "Total Timesteps: 5535 Episode Num: 392 Reward: 17.43698167434486\n",
            "Total Timesteps: 5559 Episode Num: 393 Reward: 23.351395881615463\n",
            "Total Timesteps: 5573 Episode Num: 394 Reward: 16.579038580808266\n",
            "Total Timesteps: 5595 Episode Num: 395 Reward: 22.104255042814472\n",
            "Total Timesteps: 5602 Episode Num: 396 Reward: 12.143104686099104\n",
            "Total Timesteps: 5622 Episode Num: 397 Reward: 21.217091919640374\n",
            "Total Timesteps: 5636 Episode Num: 398 Reward: 18.617940706864466\n",
            "Total Timesteps: 5644 Episode Num: 399 Reward: 13.43222403102991\n",
            "Total Timesteps: 5653 Episode Num: 400 Reward: 13.238540737693256\n",
            "Total Timesteps: 5665 Episode Num: 401 Reward: 15.390138727289742\n",
            "Total Timesteps: 5684 Episode Num: 402 Reward: 16.18658696112398\n",
            "Total Timesteps: 5700 Episode Num: 403 Reward: 18.338313495516196\n",
            "Total Timesteps: 5717 Episode Num: 404 Reward: 17.962654692125213\n",
            "Total Timesteps: 5728 Episode Num: 405 Reward: 15.743151580473931\n",
            "Total Timesteps: 5736 Episode Num: 406 Reward: 12.888714591905591\n",
            "Total Timesteps: 5746 Episode Num: 407 Reward: 14.74210353410599\n",
            "Total Timesteps: 5764 Episode Num: 408 Reward: 17.738499428451178\n",
            "Total Timesteps: 5777 Episode Num: 409 Reward: 17.215939841873478\n",
            "Total Timesteps: 5788 Episode Num: 410 Reward: 16.108249072858598\n",
            "Total Timesteps: 5798 Episode Num: 411 Reward: 15.090525535552297\n",
            "Total Timesteps: 5813 Episode Num: 412 Reward: 18.24847798246046\n",
            "Total Timesteps: 5835 Episode Num: 413 Reward: 22.41167084493063\n",
            "Total Timesteps: 5849 Episode Num: 414 Reward: 16.434528155146108\n",
            "Total Timesteps: 5866 Episode Num: 415 Reward: 21.632357920335195\n",
            "Total Timesteps: 5882 Episode Num: 416 Reward: 19.571865207981322\n",
            "Total Timesteps: 5901 Episode Num: 417 Reward: 20.680129012638638\n",
            "Total Timesteps: 5926 Episode Num: 418 Reward: 27.13234941395931\n",
            "Total Timesteps: 5935 Episode Num: 419 Reward: 13.937669787864433\n",
            "Total Timesteps: 5945 Episode Num: 420 Reward: 14.313308976410191\n",
            "Total Timesteps: 5960 Episode Num: 421 Reward: 19.164825316959472\n",
            "Total Timesteps: 5973 Episode Num: 422 Reward: 14.44669326750445\n",
            "Total Timesteps: 5983 Episode Num: 423 Reward: 14.28622337071283\n",
            "Total Timesteps: 5996 Episode Num: 424 Reward: 16.420733762645977\n",
            "Total Timesteps: 6003 Episode Num: 425 Reward: 11.302144417476663\n",
            "Total Timesteps: 6013 Episode Num: 426 Reward: 13.848168646829436\n",
            "Total Timesteps: 6023 Episode Num: 427 Reward: 14.836886484522255\n",
            "Total Timesteps: 6035 Episode Num: 428 Reward: 16.185350000981998\n",
            "Total Timesteps: 6066 Episode Num: 429 Reward: 22.25360281708709\n",
            "Total Timesteps: 6078 Episode Num: 430 Reward: 16.20284696345043\n",
            "Total Timesteps: 6090 Episode Num: 431 Reward: 15.298004151220084\n",
            "Total Timesteps: 6110 Episode Num: 432 Reward: 22.225817675091093\n",
            "Total Timesteps: 6120 Episode Num: 433 Reward: 13.558634236642682\n",
            "Total Timesteps: 6138 Episode Num: 434 Reward: 18.703292036887433\n",
            "Total Timesteps: 6147 Episode Num: 435 Reward: 12.278451070895246\n",
            "Total Timesteps: 6175 Episode Num: 436 Reward: 22.56676905667555\n",
            "Total Timesteps: 6187 Episode Num: 437 Reward: 14.593991310097044\n",
            "Total Timesteps: 6198 Episode Num: 438 Reward: 14.406388713799242\n",
            "Total Timesteps: 6211 Episode Num: 439 Reward: 14.712104202530464\n",
            "Total Timesteps: 6226 Episode Num: 440 Reward: 16.49577916502021\n",
            "Total Timesteps: 6239 Episode Num: 441 Reward: 15.573648885727742\n",
            "Total Timesteps: 6257 Episode Num: 442 Reward: 20.355805621667244\n",
            "Total Timesteps: 6279 Episode Num: 443 Reward: 22.78800191057671\n",
            "Total Timesteps: 6291 Episode Num: 444 Reward: 15.8399133868239\n",
            "Total Timesteps: 6301 Episode Num: 445 Reward: 14.693693191649801\n",
            "Total Timesteps: 6310 Episode Num: 446 Reward: 13.782152203300212\n",
            "Total Timesteps: 6324 Episode Num: 447 Reward: 16.846829687990247\n",
            "Total Timesteps: 6337 Episode Num: 448 Reward: 16.57626704722643\n",
            "Total Timesteps: 6352 Episode Num: 449 Reward: 17.597725785456714\n",
            "Total Timesteps: 6370 Episode Num: 450 Reward: 18.87573472321965\n",
            "Total Timesteps: 6386 Episode Num: 451 Reward: 20.526161221477373\n",
            "Total Timesteps: 6393 Episode Num: 452 Reward: 12.686852005621768\n",
            "Total Timesteps: 6400 Episode Num: 453 Reward: 13.358493847250065\n",
            "Total Timesteps: 6410 Episode Num: 454 Reward: 13.403467947556056\n",
            "Total Timesteps: 6419 Episode Num: 455 Reward: 13.798885671197786\n",
            "Total Timesteps: 6432 Episode Num: 456 Reward: 16.19157170199469\n",
            "Total Timesteps: 6454 Episode Num: 457 Reward: 20.660628309594173\n",
            "Total Timesteps: 6462 Episode Num: 458 Reward: 11.219350183496132\n",
            "Total Timesteps: 6469 Episode Num: 459 Reward: 12.467725163188879\n",
            "Total Timesteps: 6477 Episode Num: 460 Reward: 12.082775139811565\n",
            "Total Timesteps: 6501 Episode Num: 461 Reward: 24.23752440086682\n",
            "Total Timesteps: 6510 Episode Num: 462 Reward: 13.773115822643739\n",
            "Total Timesteps: 6532 Episode Num: 463 Reward: 20.102498010426643\n",
            "Total Timesteps: 6546 Episode Num: 464 Reward: 17.06195921757171\n",
            "Total Timesteps: 6558 Episode Num: 465 Reward: 14.722314646470474\n",
            "Total Timesteps: 6572 Episode Num: 466 Reward: 15.760172166777194\n",
            "Total Timesteps: 6586 Episode Num: 467 Reward: 16.62766777263314\n",
            "Total Timesteps: 6603 Episode Num: 468 Reward: 20.199301123608894\n",
            "Total Timesteps: 6620 Episode Num: 469 Reward: 16.5700168924086\n",
            "Total Timesteps: 6632 Episode Num: 470 Reward: 15.345023867960846\n",
            "Total Timesteps: 6642 Episode Num: 471 Reward: 14.292254933447111\n",
            "Total Timesteps: 6662 Episode Num: 472 Reward: 17.42290379171317\n",
            "Total Timesteps: 6673 Episode Num: 473 Reward: 14.30020973262581\n",
            "Total Timesteps: 6704 Episode Num: 474 Reward: 27.606084350519808\n",
            "Total Timesteps: 6717 Episode Num: 475 Reward: 17.598939951084322\n",
            "Total Timesteps: 6725 Episode Num: 476 Reward: 12.784078688795853\n",
            "Total Timesteps: 6749 Episode Num: 477 Reward: 21.064387605449888\n",
            "Total Timesteps: 6764 Episode Num: 478 Reward: 13.786037642574229\n",
            "Total Timesteps: 6774 Episode Num: 479 Reward: 14.653880552026385\n",
            "Total Timesteps: 6791 Episode Num: 480 Reward: 18.407248726514805\n",
            "Total Timesteps: 6802 Episode Num: 481 Reward: 15.703468247801355\n",
            "Total Timesteps: 6817 Episode Num: 482 Reward: 15.548903599457118\n",
            "Total Timesteps: 6851 Episode Num: 483 Reward: 28.16061017021711\n",
            "Total Timesteps: 6859 Episode Num: 484 Reward: 13.82898979436577\n",
            "Total Timesteps: 6868 Episode Num: 485 Reward: 13.761651863055883\n",
            "Total Timesteps: 6877 Episode Num: 486 Reward: 13.811902746229315\n",
            "Total Timesteps: 6890 Episode Num: 487 Reward: 15.924853413570963\n",
            "Total Timesteps: 6899 Episode Num: 488 Reward: 11.28377992491296\n",
            "Total Timesteps: 6909 Episode Num: 489 Reward: 15.228546541376272\n",
            "Total Timesteps: 6931 Episode Num: 490 Reward: 20.145617475586192\n",
            "Total Timesteps: 6940 Episode Num: 491 Reward: 12.893837399967016\n",
            "Total Timesteps: 6949 Episode Num: 492 Reward: 13.278641863580559\n",
            "Total Timesteps: 6965 Episode Num: 493 Reward: 19.63333210330893\n",
            "Total Timesteps: 6975 Episode Num: 494 Reward: 14.760853056400082\n",
            "Total Timesteps: 6989 Episode Num: 495 Reward: 18.71647872466856\n",
            "Total Timesteps: 6999 Episode Num: 496 Reward: 14.798762447835177\n",
            "Total Timesteps: 7015 Episode Num: 497 Reward: 16.99979124608362\n",
            "Total Timesteps: 7039 Episode Num: 498 Reward: 21.38726140592189\n",
            "Total Timesteps: 7051 Episode Num: 499 Reward: 14.075554900761926\n",
            "Total Timesteps: 7060 Episode Num: 500 Reward: 13.52266435534839\n",
            "Total Timesteps: 7075 Episode Num: 501 Reward: 18.007164614969223\n",
            "Total Timesteps: 7085 Episode Num: 502 Reward: 14.049109010404207\n",
            "Total Timesteps: 7096 Episode Num: 503 Reward: 16.147819881419128\n",
            "Total Timesteps: 7113 Episode Num: 504 Reward: 20.243508337772802\n",
            "Total Timesteps: 7127 Episode Num: 505 Reward: 13.914176723189302\n",
            "Total Timesteps: 7138 Episode Num: 506 Reward: 16.174000276118747\n",
            "Total Timesteps: 7158 Episode Num: 507 Reward: 24.011069646399125\n",
            "Total Timesteps: 7176 Episode Num: 508 Reward: 21.58857989524986\n",
            "Total Timesteps: 7184 Episode Num: 509 Reward: 13.591955578506166\n",
            "Total Timesteps: 7199 Episode Num: 510 Reward: 16.33085815872473\n",
            "Total Timesteps: 7209 Episode Num: 511 Reward: 13.248999343361357\n",
            "Total Timesteps: 7218 Episode Num: 512 Reward: 13.302604782088018\n",
            "Total Timesteps: 7228 Episode Num: 513 Reward: 13.974021987162995\n",
            "Total Timesteps: 7238 Episode Num: 514 Reward: 15.503384840501528\n",
            "Total Timesteps: 7253 Episode Num: 515 Reward: 14.526567118569801\n",
            "Total Timesteps: 7266 Episode Num: 516 Reward: 17.690496958355656\n",
            "Total Timesteps: 7287 Episode Num: 517 Reward: 19.4640348839981\n",
            "Total Timesteps: 7309 Episode Num: 518 Reward: 20.670759838160304\n",
            "Total Timesteps: 7325 Episode Num: 519 Reward: 17.063960706628855\n",
            "Total Timesteps: 7349 Episode Num: 520 Reward: 23.073744791439093\n",
            "Total Timesteps: 7362 Episode Num: 521 Reward: 16.083528686060163\n",
            "Total Timesteps: 7374 Episode Num: 522 Reward: 14.968305137351855\n",
            "Total Timesteps: 7394 Episode Num: 523 Reward: 19.90919515410787\n",
            "Total Timesteps: 7412 Episode Num: 524 Reward: 21.71889658224827\n",
            "Total Timesteps: 7428 Episode Num: 525 Reward: 18.23030071777466\n",
            "Total Timesteps: 7453 Episode Num: 526 Reward: 28.434124601200164\n",
            "Total Timesteps: 7462 Episode Num: 527 Reward: 14.443080501172517\n",
            "Total Timesteps: 7476 Episode Num: 528 Reward: 19.01375461676798\n",
            "Total Timesteps: 7494 Episode Num: 529 Reward: 19.36125548771961\n",
            "Total Timesteps: 7514 Episode Num: 530 Reward: 17.607752516305478\n",
            "Total Timesteps: 7530 Episode Num: 531 Reward: 18.942369008713285\n",
            "Total Timesteps: 7543 Episode Num: 532 Reward: 14.37185529857088\n",
            "Total Timesteps: 7553 Episode Num: 533 Reward: 13.829323264941921\n",
            "Total Timesteps: 7577 Episode Num: 534 Reward: 22.560963929217543\n",
            "Total Timesteps: 7591 Episode Num: 535 Reward: 17.345699516913733\n",
            "Total Timesteps: 7606 Episode Num: 536 Reward: 17.39768509668211\n",
            "Total Timesteps: 7615 Episode Num: 537 Reward: 13.211031344506774\n",
            "Total Timesteps: 7630 Episode Num: 538 Reward: 18.2630365256744\n",
            "Total Timesteps: 7640 Episode Num: 539 Reward: 14.826182845371658\n",
            "Total Timesteps: 7653 Episode Num: 540 Reward: 15.792451983562206\n",
            "Total Timesteps: 7661 Episode Num: 541 Reward: 12.047664194808748\n",
            "Total Timesteps: 7674 Episode Num: 542 Reward: 16.287960690780892\n",
            "Total Timesteps: 7685 Episode Num: 543 Reward: 13.788407744556025\n",
            "Total Timesteps: 7706 Episode Num: 544 Reward: 19.535531208739847\n",
            "Total Timesteps: 7719 Episode Num: 545 Reward: 16.536342590382265\n",
            "Total Timesteps: 7733 Episode Num: 546 Reward: 17.507949335855663\n",
            "Total Timesteps: 7747 Episode Num: 547 Reward: 17.366431576194007\n",
            "Total Timesteps: 7759 Episode Num: 548 Reward: 16.18642635286087\n",
            "Total Timesteps: 7773 Episode Num: 549 Reward: 14.577927859792544\n",
            "Total Timesteps: 7790 Episode Num: 550 Reward: 19.227890814733108\n",
            "Total Timesteps: 7801 Episode Num: 551 Reward: 15.139137049560667\n",
            "Total Timesteps: 7810 Episode Num: 552 Reward: 13.305772871205408\n",
            "Total Timesteps: 7821 Episode Num: 553 Reward: 14.691518695744161\n",
            "Total Timesteps: 7833 Episode Num: 554 Reward: 16.000475602180813\n",
            "Total Timesteps: 7844 Episode Num: 555 Reward: 15.554452545204548\n",
            "Total Timesteps: 7867 Episode Num: 556 Reward: 21.513912247844562\n",
            "Total Timesteps: 7887 Episode Num: 557 Reward: 22.970838980621192\n",
            "Total Timesteps: 7898 Episode Num: 558 Reward: 15.67972395084944\n",
            "Total Timesteps: 7911 Episode Num: 559 Reward: 17.89164517920726\n",
            "Total Timesteps: 7931 Episode Num: 560 Reward: 21.018951193950485\n",
            "Total Timesteps: 7951 Episode Num: 561 Reward: 20.79454384160781\n",
            "Total Timesteps: 7976 Episode Num: 562 Reward: 23.467951220819664\n",
            "Total Timesteps: 7995 Episode Num: 563 Reward: 18.044158744101875\n",
            "Total Timesteps: 8018 Episode Num: 564 Reward: 24.301071469784077\n",
            "Total Timesteps: 8031 Episode Num: 565 Reward: 16.588477134399\n",
            "Total Timesteps: 8047 Episode Num: 566 Reward: 17.158110601388035\n",
            "Total Timesteps: 8062 Episode Num: 567 Reward: 17.472999376020745\n",
            "Total Timesteps: 8072 Episode Num: 568 Reward: 15.978765305674461\n",
            "Total Timesteps: 8092 Episode Num: 569 Reward: 20.981843183700402\n",
            "Total Timesteps: 8107 Episode Num: 570 Reward: 18.52650967593509\n",
            "Total Timesteps: 8130 Episode Num: 571 Reward: 18.97681057537557\n",
            "Total Timesteps: 8138 Episode Num: 572 Reward: 11.945458039654476\n",
            "Total Timesteps: 8148 Episode Num: 573 Reward: 15.358302986902707\n",
            "Total Timesteps: 8167 Episode Num: 574 Reward: 22.11225123660406\n",
            "Total Timesteps: 8183 Episode Num: 575 Reward: 18.37923711875192\n",
            "Total Timesteps: 8192 Episode Num: 576 Reward: 13.472608362752363\n",
            "Total Timesteps: 8209 Episode Num: 577 Reward: 19.309606656002867\n",
            "Total Timesteps: 8223 Episode Num: 578 Reward: 14.896372916216203\n",
            "Total Timesteps: 8243 Episode Num: 579 Reward: 20.39848110432213\n",
            "Total Timesteps: 8257 Episode Num: 580 Reward: 15.660384361079196\n",
            "Total Timesteps: 8275 Episode Num: 581 Reward: 18.428626173519298\n",
            "Total Timesteps: 8295 Episode Num: 582 Reward: 23.000681471358984\n",
            "Total Timesteps: 8303 Episode Num: 583 Reward: 12.55392749676248\n",
            "Total Timesteps: 8326 Episode Num: 584 Reward: 21.00205948562653\n",
            "Total Timesteps: 8337 Episode Num: 585 Reward: 14.610681889043189\n",
            "Total Timesteps: 8359 Episode Num: 586 Reward: 19.858547819140945\n",
            "Total Timesteps: 8367 Episode Num: 587 Reward: 11.827395585105112\n",
            "Total Timesteps: 8376 Episode Num: 588 Reward: 13.515885972663819\n",
            "Total Timesteps: 8387 Episode Num: 589 Reward: 13.548108424575183\n",
            "Total Timesteps: 8395 Episode Num: 590 Reward: 11.990727405570214\n",
            "Total Timesteps: 8407 Episode Num: 591 Reward: 15.835657672068919\n",
            "Total Timesteps: 8427 Episode Num: 592 Reward: 20.67343898766849\n",
            "Total Timesteps: 8437 Episode Num: 593 Reward: 15.705123653833288\n",
            "Total Timesteps: 8449 Episode Num: 594 Reward: 15.57717392053746\n",
            "Total Timesteps: 8467 Episode Num: 595 Reward: 17.783528590948844\n",
            "Total Timesteps: 8491 Episode Num: 596 Reward: 23.224543811602054\n",
            "Total Timesteps: 8505 Episode Num: 597 Reward: 17.133797575466453\n",
            "Total Timesteps: 8519 Episode Num: 598 Reward: 15.881461242280785\n",
            "Total Timesteps: 8531 Episode Num: 599 Reward: 12.812268496165052\n",
            "Total Timesteps: 8543 Episode Num: 600 Reward: 14.911638881391264\n",
            "Total Timesteps: 8551 Episode Num: 601 Reward: 13.476527779872413\n",
            "Total Timesteps: 8580 Episode Num: 602 Reward: 27.383241705775433\n",
            "Total Timesteps: 8595 Episode Num: 603 Reward: 17.83204249658738\n",
            "Total Timesteps: 8614 Episode Num: 604 Reward: 19.483208761129934\n",
            "Total Timesteps: 8622 Episode Num: 605 Reward: 12.135099647061724\n",
            "Total Timesteps: 8631 Episode Num: 606 Reward: 14.665583757286367\n",
            "Total Timesteps: 8643 Episode Num: 607 Reward: 16.064409880529272\n",
            "Total Timesteps: 8656 Episode Num: 608 Reward: 18.144857504173704\n",
            "Total Timesteps: 8676 Episode Num: 609 Reward: 21.726574236669695\n",
            "Total Timesteps: 8700 Episode Num: 610 Reward: 22.506144723510076\n",
            "Total Timesteps: 8709 Episode Num: 611 Reward: 12.675056763483735\n",
            "Total Timesteps: 8726 Episode Num: 612 Reward: 18.07422306819062\n",
            "Total Timesteps: 8734 Episode Num: 613 Reward: 13.432689197390573\n",
            "Total Timesteps: 8757 Episode Num: 614 Reward: 19.96954145440832\n",
            "Total Timesteps: 8767 Episode Num: 615 Reward: 12.968119776013191\n",
            "Total Timesteps: 8780 Episode Num: 616 Reward: 16.340745214211346\n",
            "Total Timesteps: 8790 Episode Num: 617 Reward: 15.434989587400926\n",
            "Total Timesteps: 8819 Episode Num: 618 Reward: 27.65699432295805\n",
            "Total Timesteps: 8827 Episode Num: 619 Reward: 12.57816359652352\n",
            "Total Timesteps: 8842 Episode Num: 620 Reward: 17.370096225239102\n",
            "Total Timesteps: 8851 Episode Num: 621 Reward: 13.57168615467672\n",
            "Total Timesteps: 8862 Episode Num: 622 Reward: 14.462631641389452\n",
            "Total Timesteps: 8869 Episode Num: 623 Reward: 12.062382441351655\n",
            "Total Timesteps: 8882 Episode Num: 624 Reward: 15.837156005155702\n",
            "Total Timesteps: 8896 Episode Num: 625 Reward: 16.733521633711643\n",
            "Total Timesteps: 8912 Episode Num: 626 Reward: 19.2536156976872\n",
            "Total Timesteps: 8922 Episode Num: 627 Reward: 14.613873837132997\n",
            "Total Timesteps: 8932 Episode Num: 628 Reward: 12.370071679666578\n",
            "Total Timesteps: 8954 Episode Num: 629 Reward: 20.259206234359596\n",
            "Total Timesteps: 8971 Episode Num: 630 Reward: 18.284454354232004\n",
            "Total Timesteps: 8989 Episode Num: 631 Reward: 17.03072415271308\n",
            "Total Timesteps: 9006 Episode Num: 632 Reward: 19.716162533426544\n",
            "Total Timesteps: 9030 Episode Num: 633 Reward: 27.43466103678656\n",
            "Total Timesteps: 9038 Episode Num: 634 Reward: 12.353720690446789\n",
            "Total Timesteps: 9048 Episode Num: 635 Reward: 13.167044639900267\n",
            "Total Timesteps: 9070 Episode Num: 636 Reward: 19.519367709706422\n",
            "Total Timesteps: 9080 Episode Num: 637 Reward: 13.698636757954956\n",
            "Total Timesteps: 9100 Episode Num: 638 Reward: 20.968938981708195\n",
            "Total Timesteps: 9117 Episode Num: 639 Reward: 19.814511585139556\n",
            "Total Timesteps: 9132 Episode Num: 640 Reward: 14.791679015394767\n",
            "Total Timesteps: 9148 Episode Num: 641 Reward: 18.134932944715548\n",
            "Total Timesteps: 9168 Episode Num: 642 Reward: 21.473153661437387\n",
            "Total Timesteps: 9177 Episode Num: 643 Reward: 13.327448831545189\n",
            "Total Timesteps: 9187 Episode Num: 644 Reward: 14.132063561222457\n",
            "Total Timesteps: 9202 Episode Num: 645 Reward: 15.757162116406834\n",
            "Total Timesteps: 9214 Episode Num: 646 Reward: 14.67814315785072\n",
            "Total Timesteps: 9233 Episode Num: 647 Reward: 19.532207486235713\n",
            "Total Timesteps: 9249 Episode Num: 648 Reward: 19.272935847082408\n",
            "Total Timesteps: 9268 Episode Num: 649 Reward: 18.771353892654584\n",
            "Total Timesteps: 9279 Episode Num: 650 Reward: 14.811605235451133\n",
            "Total Timesteps: 9294 Episode Num: 651 Reward: 20.266970704427514\n",
            "Total Timesteps: 9302 Episode Num: 652 Reward: 11.06270642208692\n",
            "Total Timesteps: 9313 Episode Num: 653 Reward: 17.094591655266413\n",
            "Total Timesteps: 9322 Episode Num: 654 Reward: 12.897669499489712\n",
            "Total Timesteps: 9332 Episode Num: 655 Reward: 14.264417551324003\n",
            "Total Timesteps: 9342 Episode Num: 656 Reward: 13.758497579376852\n",
            "Total Timesteps: 9354 Episode Num: 657 Reward: 15.666891717711403\n",
            "Total Timesteps: 9367 Episode Num: 658 Reward: 14.464526619121898\n",
            "Total Timesteps: 9382 Episode Num: 659 Reward: 18.33908825297258\n",
            "Total Timesteps: 9398 Episode Num: 660 Reward: 16.180666472757\n",
            "Total Timesteps: 9412 Episode Num: 661 Reward: 16.666143813474626\n",
            "Total Timesteps: 9432 Episode Num: 662 Reward: 16.93303841496672\n",
            "Total Timesteps: 9447 Episode Num: 663 Reward: 15.4958219600172\n",
            "Total Timesteps: 9467 Episode Num: 664 Reward: 17.427084921598727\n",
            "Total Timesteps: 9478 Episode Num: 665 Reward: 14.246825087645263\n",
            "Total Timesteps: 9491 Episode Num: 666 Reward: 15.881268436195384\n",
            "Total Timesteps: 9516 Episode Num: 667 Reward: 21.887736540200418\n",
            "Total Timesteps: 9528 Episode Num: 668 Reward: 14.689158289242187\n",
            "Total Timesteps: 9541 Episode Num: 669 Reward: 17.058794828115786\n",
            "Total Timesteps: 9558 Episode Num: 670 Reward: 18.82212481834431\n",
            "Total Timesteps: 9566 Episode Num: 671 Reward: 14.511243558219576\n",
            "Total Timesteps: 9578 Episode Num: 672 Reward: 14.770668331735942\n",
            "Total Timesteps: 9600 Episode Num: 673 Reward: 24.655260063508468\n",
            "Total Timesteps: 9615 Episode Num: 674 Reward: 19.273124284321966\n",
            "Total Timesteps: 9625 Episode Num: 675 Reward: 15.83803428289539\n",
            "Total Timesteps: 9646 Episode Num: 676 Reward: 23.05703954988712\n",
            "Total Timesteps: 9654 Episode Num: 677 Reward: 13.039039211240015\n",
            "Total Timesteps: 9664 Episode Num: 678 Reward: 16.022539229487304\n",
            "Total Timesteps: 9677 Episode Num: 679 Reward: 16.3996844812762\n",
            "Total Timesteps: 9699 Episode Num: 680 Reward: 24.231765572365834\n",
            "Total Timesteps: 9711 Episode Num: 681 Reward: 14.52547284863249\n",
            "Total Timesteps: 9722 Episode Num: 682 Reward: 15.480313431899411\n",
            "Total Timesteps: 9733 Episode Num: 683 Reward: 14.501968749875958\n",
            "Total Timesteps: 9761 Episode Num: 684 Reward: 20.282530402975684\n",
            "Total Timesteps: 9770 Episode Num: 685 Reward: 13.765932527511904\n",
            "Total Timesteps: 9789 Episode Num: 686 Reward: 20.33557689742156\n",
            "Total Timesteps: 9801 Episode Num: 687 Reward: 15.99605381024303\n",
            "Total Timesteps: 9812 Episode Num: 688 Reward: 14.234347313232137\n",
            "Total Timesteps: 9834 Episode Num: 689 Reward: 20.923803684215816\n",
            "Total Timesteps: 9843 Episode Num: 690 Reward: 15.919840224356449\n",
            "Total Timesteps: 9860 Episode Num: 691 Reward: 18.359782616935263\n",
            "Total Timesteps: 9874 Episode Num: 692 Reward: 16.334371798372015\n",
            "Total Timesteps: 9885 Episode Num: 693 Reward: 14.008832541685843\n",
            "Total Timesteps: 9915 Episode Num: 694 Reward: 31.951465932131395\n",
            "Total Timesteps: 9932 Episode Num: 695 Reward: 17.316680412935966\n",
            "Total Timesteps: 9941 Episode Num: 696 Reward: 13.193497770314568\n",
            "Total Timesteps: 9958 Episode Num: 697 Reward: 14.161254164626005\n",
            "Total Timesteps: 9966 Episode Num: 698 Reward: 12.903015262135884\n",
            "Total Timesteps: 9991 Episode Num: 699 Reward: 22.205284348130228\n",
            "Total Timesteps: 10056 Episode Num: 700 Reward: 76.94177904976881\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 53.996154\n",
            "---------------------------------------\n",
            "Total Timesteps: 10122 Episode Num: 701 Reward: 25.721538667691245\n",
            "Total Timesteps: 10187 Episode Num: 702 Reward: 32.48595966385163\n",
            "Total Timesteps: 10249 Episode Num: 703 Reward: 76.78337857981343\n",
            "Total Timesteps: 10292 Episode Num: 704 Reward: 52.9881702217879\n",
            "Total Timesteps: 10378 Episode Num: 705 Reward: 56.17124634348399\n",
            "Total Timesteps: 10435 Episode Num: 706 Reward: 67.59576206421445\n",
            "Total Timesteps: 10492 Episode Num: 707 Reward: 67.13440058612444\n",
            "Total Timesteps: 10550 Episode Num: 708 Reward: 67.70793292980086\n",
            "Total Timesteps: 10574 Episode Num: 709 Reward: 24.124683986387105\n",
            "Total Timesteps: 10639 Episode Num: 710 Reward: 82.65439234884894\n",
            "Total Timesteps: 10657 Episode Num: 711 Reward: 19.999639277721105\n",
            "Total Timesteps: 10707 Episode Num: 712 Reward: 59.07327541015367\n",
            "Total Timesteps: 10767 Episode Num: 713 Reward: 76.97184618614213\n",
            "Total Timesteps: 10803 Episode Num: 714 Reward: 35.83156411249815\n",
            "Total Timesteps: 10867 Episode Num: 715 Reward: 79.16950331940872\n",
            "Total Timesteps: 10919 Episode Num: 716 Reward: 72.57213425332763\n",
            "Total Timesteps: 11011 Episode Num: 717 Reward: 102.96315088760163\n",
            "Total Timesteps: 11042 Episode Num: 718 Reward: 30.45718696244681\n",
            "Total Timesteps: 11099 Episode Num: 719 Reward: 71.86135377425323\n",
            "Total Timesteps: 11141 Episode Num: 720 Reward: 57.64719226084498\n",
            "Total Timesteps: 11185 Episode Num: 721 Reward: 40.715630841238244\n",
            "Total Timesteps: 11238 Episode Num: 722 Reward: 65.14937379197718\n",
            "Total Timesteps: 11283 Episode Num: 723 Reward: 56.339950606982136\n",
            "Total Timesteps: 11320 Episode Num: 724 Reward: 45.05345521648174\n",
            "Total Timesteps: 11390 Episode Num: 725 Reward: 78.97688801288541\n",
            "Total Timesteps: 11448 Episode Num: 726 Reward: 69.4488174763293\n",
            "Total Timesteps: 11494 Episode Num: 727 Reward: 54.09184185314376\n",
            "Total Timesteps: 11582 Episode Num: 728 Reward: 98.8443271921386\n",
            "Total Timesteps: 11630 Episode Num: 729 Reward: 61.82884386436828\n",
            "Total Timesteps: 11676 Episode Num: 730 Reward: 47.60006881348603\n",
            "Total Timesteps: 11758 Episode Num: 731 Reward: 26.78505191913194\n",
            "Total Timesteps: 11834 Episode Num: 732 Reward: 85.76567098006988\n",
            "Total Timesteps: 11878 Episode Num: 733 Reward: 55.04078094658793\n",
            "Total Timesteps: 11920 Episode Num: 734 Reward: 41.40148740702219\n",
            "Total Timesteps: 11974 Episode Num: 735 Reward: 61.36740567170433\n",
            "Total Timesteps: 12039 Episode Num: 736 Reward: 16.33205212466143\n",
            "Total Timesteps: 12096 Episode Num: 737 Reward: 70.51056104540594\n",
            "Total Timesteps: 12146 Episode Num: 738 Reward: 57.92151010536131\n",
            "Total Timesteps: 12227 Episode Num: 739 Reward: 87.71285827472262\n",
            "Total Timesteps: 12324 Episode Num: 740 Reward: 92.30127013692642\n",
            "Total Timesteps: 12377 Episode Num: 741 Reward: 65.1559975986436\n",
            "Total Timesteps: 12428 Episode Num: 742 Reward: 65.82157093006904\n",
            "Total Timesteps: 12488 Episode Num: 743 Reward: 69.77594753307194\n",
            "Total Timesteps: 12540 Episode Num: 744 Reward: 63.97818699985634\n",
            "Total Timesteps: 12587 Episode Num: 745 Reward: 59.90532180885607\n",
            "Total Timesteps: 12659 Episode Num: 746 Reward: 78.00208902234226\n",
            "Total Timesteps: 12727 Episode Num: 747 Reward: 79.76089790393468\n",
            "Total Timesteps: 12781 Episode Num: 748 Reward: 60.426023421692584\n",
            "Total Timesteps: 12833 Episode Num: 749 Reward: 65.90481709017466\n",
            "Total Timesteps: 12916 Episode Num: 750 Reward: 87.0048921763577\n",
            "Total Timesteps: 12970 Episode Num: 751 Reward: 69.97919815744443\n",
            "Total Timesteps: 13021 Episode Num: 752 Reward: 65.82838636859398\n",
            "Total Timesteps: 13066 Episode Num: 753 Reward: 60.712764823455274\n",
            "Total Timesteps: 13115 Episode Num: 754 Reward: 65.04212985535744\n",
            "Total Timesteps: 13161 Episode Num: 755 Reward: 58.40599862829961\n",
            "Total Timesteps: 13216 Episode Num: 756 Reward: 66.4304204622989\n",
            "Total Timesteps: 13269 Episode Num: 757 Reward: 71.53234098412327\n",
            "Total Timesteps: 13335 Episode Num: 758 Reward: 72.96668788864918\n",
            "Total Timesteps: 13386 Episode Num: 759 Reward: 65.78310158548575\n",
            "Total Timesteps: 13432 Episode Num: 760 Reward: 59.772932746996936\n",
            "Total Timesteps: 13498 Episode Num: 761 Reward: 77.67357605848522\n",
            "Total Timesteps: 13534 Episode Num: 762 Reward: 27.29682801484792\n",
            "Total Timesteps: 13582 Episode Num: 763 Reward: 58.9717013216848\n",
            "Total Timesteps: 13632 Episode Num: 764 Reward: 62.74554392017708\n",
            "Total Timesteps: 13698 Episode Num: 765 Reward: 79.665534008348\n",
            "Total Timesteps: 13749 Episode Num: 766 Reward: 68.69634042268957\n",
            "Total Timesteps: 13800 Episode Num: 767 Reward: 68.55800456133265\n",
            "Total Timesteps: 13886 Episode Num: 768 Reward: 54.39571704274338\n",
            "Total Timesteps: 13941 Episode Num: 769 Reward: 59.57040475065559\n",
            "Total Timesteps: 14016 Episode Num: 770 Reward: 83.36553450542068\n",
            "Total Timesteps: 14081 Episode Num: 771 Reward: 69.31267284796145\n",
            "Total Timesteps: 14142 Episode Num: 772 Reward: 71.4193050476158\n",
            "Total Timesteps: 14214 Episode Num: 773 Reward: 81.77305027912125\n",
            "Total Timesteps: 14247 Episode Num: 774 Reward: 32.35387741928886\n",
            "Total Timesteps: 14291 Episode Num: 775 Reward: 40.21297764132946\n",
            "Total Timesteps: 14340 Episode Num: 776 Reward: 56.486556978722064\n",
            "Total Timesteps: 14401 Episode Num: 777 Reward: 73.83188089586075\n",
            "Total Timesteps: 14463 Episode Num: 778 Reward: 69.66620393149027\n",
            "Total Timesteps: 14492 Episode Num: 779 Reward: 24.48165912945292\n",
            "Total Timesteps: 14548 Episode Num: 780 Reward: 68.59253824813354\n",
            "Total Timesteps: 14603 Episode Num: 781 Reward: 69.20633996074775\n",
            "Total Timesteps: 14673 Episode Num: 782 Reward: 72.0474329428813\n",
            "Total Timesteps: 14737 Episode Num: 783 Reward: 74.13069331985234\n",
            "Total Timesteps: 14806 Episode Num: 784 Reward: 74.44928380391528\n",
            "Total Timesteps: 14861 Episode Num: 785 Reward: 61.782380297756376\n",
            "Total Timesteps: 14924 Episode Num: 786 Reward: 70.95261119688615\n",
            "Total Timesteps: 14992 Episode Num: 787 Reward: 76.87693789468636\n",
            "Total Timesteps: 15025 Episode Num: 788 Reward: 35.410777836009686\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 54.651374\n",
            "---------------------------------------\n",
            "Total Timesteps: 15085 Episode Num: 789 Reward: 70.02279882006712\n",
            "Total Timesteps: 15138 Episode Num: 790 Reward: 63.49880070943516\n",
            "Total Timesteps: 15168 Episode Num: 791 Reward: 22.409538101180374\n",
            "Total Timesteps: 15240 Episode Num: 792 Reward: 78.02977174867075\n",
            "Total Timesteps: 15316 Episode Num: 793 Reward: 50.43962995315504\n",
            "Total Timesteps: 15394 Episode Num: 794 Reward: 85.58965204721477\n",
            "Total Timesteps: 15423 Episode Num: 795 Reward: 27.78359561099154\n",
            "Total Timesteps: 15482 Episode Num: 796 Reward: 66.77988587831281\n",
            "Total Timesteps: 15542 Episode Num: 797 Reward: 72.78779217448685\n",
            "Total Timesteps: 15591 Episode Num: 798 Reward: 67.33148715077579\n",
            "Total Timesteps: 15657 Episode Num: 799 Reward: 79.2397410879312\n",
            "Total Timesteps: 15761 Episode Num: 800 Reward: 97.64044014287141\n",
            "Total Timesteps: 15853 Episode Num: 801 Reward: 40.74691942864723\n",
            "Total Timesteps: 15925 Episode Num: 802 Reward: 76.12657121121617\n",
            "Total Timesteps: 15982 Episode Num: 803 Reward: 69.89595694362478\n",
            "Total Timesteps: 16047 Episode Num: 804 Reward: 74.04412319376083\n",
            "Total Timesteps: 16113 Episode Num: 805 Reward: 77.56151099165979\n",
            "Total Timesteps: 16180 Episode Num: 806 Reward: 80.99238212295788\n",
            "Total Timesteps: 16258 Episode Num: 807 Reward: 86.5851288520724\n",
            "Total Timesteps: 16339 Episode Num: 808 Reward: 86.40345317849639\n",
            "Total Timesteps: 16413 Episode Num: 809 Reward: 88.19411222850464\n",
            "Total Timesteps: 16491 Episode Num: 810 Reward: 31.854065072746177\n",
            "Total Timesteps: 16553 Episode Num: 811 Reward: 74.26522819193484\n",
            "Total Timesteps: 16638 Episode Num: 812 Reward: 95.05676266737316\n",
            "Total Timesteps: 16698 Episode Num: 813 Reward: 78.6512856299591\n",
            "Total Timesteps: 16772 Episode Num: 814 Reward: 53.069471450350655\n",
            "Total Timesteps: 16869 Episode Num: 815 Reward: 109.34526249914359\n",
            "Total Timesteps: 16940 Episode Num: 816 Reward: 57.38742421984175\n",
            "Total Timesteps: 17009 Episode Num: 817 Reward: 93.00961536522676\n",
            "Total Timesteps: 17080 Episode Num: 818 Reward: 89.62302679467248\n",
            "Total Timesteps: 17135 Episode Num: 819 Reward: 71.1294708620362\n",
            "Total Timesteps: 17242 Episode Num: 820 Reward: 112.63119222404039\n",
            "Total Timesteps: 17306 Episode Num: 821 Reward: 76.68852978591404\n",
            "Total Timesteps: 17375 Episode Num: 822 Reward: 91.70480602146472\n",
            "Total Timesteps: 17436 Episode Num: 823 Reward: 80.56657105173632\n",
            "Total Timesteps: 17503 Episode Num: 824 Reward: 81.1013752892303\n",
            "Total Timesteps: 17572 Episode Num: 825 Reward: 89.36942351303446\n",
            "Total Timesteps: 17635 Episode Num: 826 Reward: 79.75290665296914\n",
            "Total Timesteps: 17694 Episode Num: 827 Reward: 66.43024390950502\n",
            "Total Timesteps: 17768 Episode Num: 828 Reward: 90.23871981165571\n",
            "Total Timesteps: 17845 Episode Num: 829 Reward: 88.3211170358938\n",
            "Total Timesteps: 17913 Episode Num: 830 Reward: 86.53689332662981\n",
            "Total Timesteps: 18005 Episode Num: 831 Reward: 109.96942373386666\n",
            "Total Timesteps: 18061 Episode Num: 832 Reward: 56.017741164915876\n",
            "Total Timesteps: 18123 Episode Num: 833 Reward: 70.05206893507871\n",
            "Total Timesteps: 18213 Episode Num: 834 Reward: 99.36465285793466\n",
            "Total Timesteps: 18307 Episode Num: 835 Reward: 105.89789956472235\n",
            "Total Timesteps: 18374 Episode Num: 836 Reward: 75.49265769400716\n",
            "Total Timesteps: 18469 Episode Num: 837 Reward: 106.34850993920287\n",
            "Total Timesteps: 18596 Episode Num: 838 Reward: 133.9195783347916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-14812cd6e887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total Timesteps: {} Episode Num: {} Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# We evaluate the episode and we save the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-da16fa8c629e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "e7559a94-b0da-4f77-ef9c-084cffab8cd3"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"Walker2DBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_Walker2DBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 60.181348\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcnexWrW4a8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}